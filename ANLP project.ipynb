{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92885db",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "676ea8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e746c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#to open csv file\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#sentences & words tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#regular expression \n",
    "import re\n",
    "#for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem import ISRIStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "feb6d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ANTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ANTER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f712df6",
   "metadata": {},
   "source": [
    "# csv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "edc8806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arabic</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>متى أنشئت هذه الجامعة؟</td>\n",
       "      <td>When was this university founded?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>أراها نادراً</td>\n",
       "      <td>I see it rarely.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>يعزف على البيانو بشكل جيد جداً</td>\n",
       "      <td>He plays the piano very well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مع كل احترامي.</td>\n",
       "      <td>With all due respect.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نظف أسنانك</td>\n",
       "      <td>Brush your teeth clean.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           arabic                            english\n",
       "0          متى أنشئت هذه الجامعة؟  When was this university founded?\n",
       "1                    أراها نادراً                   I see it rarely.\n",
       "2  يعزف على البيانو بشكل جيد جداً      He plays the piano very well.\n",
       "3                  مع كل احترامي.              With all due respect.\n",
       "4                      نظف أسنانك            Brush your teeth clean."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = r'C:\\Users\\ANTER\\Downloads\\arabic_english.csv'\n",
    "dataset = pd.read_csv(dataset_path, encoding=\"utf-8\")\n",
    "dataset = dataset.head(5000) \n",
    "max_len = 30 \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a7056",
   "metadata": {},
   "source": [
    "# preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6fe42",
   "metadata": {},
   "source": [
    "### arabic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f109601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Apply lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove Arabic stopwords\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Remove Arabic punctuation and other non-alphanumeric characters\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "    \n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    \n",
    "    # Apply stemming (no need for lemmatization for Arabic)\n",
    "    stemmer = ISRIStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d98d0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['arabic_preprocessed'] = dataset['arabic'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c1cd216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      [شئت, جمع]\n",
       "1                      [ارا, ندر]\n",
       "2       [عزف, ينو, شكل, جيد, جدا]\n",
       "3                           [حرم]\n",
       "4                      [نظف, سنن]\n",
       "                  ...            \n",
       "4995         [سفر, ابي, خرج, احا]\n",
       "4996         [ابي, غضب, مني, جدا]\n",
       "4997         [يحب, ابي, يتز, كثر]\n",
       "4998         [سبق, لأب, سفر, خرج]\n",
       "4999              [سفر, ابي, خرج]\n",
       "Name: arabic_preprocessed, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['arabic_preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e09b2",
   "metadata": {},
   "source": [
    "### english preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1da50341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_english_text(text):\n",
    "    # Apply lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Remove English punctuation and other non-alphanumeric characters\n",
    "    tokens = [re.sub(r'[^a-zA-Z]', '', token) for token in tokens]\n",
    "    # Remove empty tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5176fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_preprocessed'] = dataset['english'].apply(preprocess_english_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "579ef3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 [university, founded]\n",
       "1                         [see, rarely]\n",
       "2                   [play, piano, well]\n",
       "3                        [due, respect]\n",
       "4                 [brush, teeth, clean]\n",
       "                     ...               \n",
       "4995    [father, sometimes, go, abroad]\n",
       "4996                    [father, angry]\n",
       "4997        [father, like, pizza, much]\n",
       "4998            [father, never, abroad]\n",
       "4999            [father, never, abroad]\n",
       "Name: english_preprocessed, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['english_preprocessed']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76736404",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd6b2f",
   "metadata": {},
   "source": [
    "### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ec2487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming you have lists of tokenized Arabic and English sentences\n",
    "arabic_corpus = [sentence for sentence in dataset['arabic_preprocessed']]\n",
    "english_corpus = [sentence for sentence in dataset['english_preprocessed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f46e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Arabic Word2Vec model\n",
    "arabic_model = Word2Vec(sentences=arabic_corpus, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Train English Word2Vec model\n",
    "english_model = Word2Vec(sentences=english_corpus, vector_size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "588322e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_model.save('arabic_word2vec.model')\n",
    "english_model.save('english_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "89becdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "arabic_model = Word2Vec.load('arabic_word2vec.model')\n",
    "english_model = Word2Vec.load('english_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf8d030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_embeddings(text, model):\n",
    "    embeddings = []\n",
    "    for word in text:\n",
    "        if word in model.wv:\n",
    "            embeddings.append(model.wv[word])\n",
    "        else:\n",
    "            # Handle out-of-vocabulary words\n",
    "            embeddings.append(np.zeros(model.vector_size))\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f49b06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['arabic_embeddings'] = dataset['arabic_preprocessed'].apply(lambda x: text_to_embeddings(x, arabic_model))\n",
    "dataset['english_embeddings'] = dataset['english_preprocessed'].apply(lambda x: text_to_embeddings(x, english_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16cc25da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "1       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "2       [[-0.005348652, 0.015084333, 0.0045133135, 0.0...\n",
       "3       [[-0.009835003, 0.001242528, -0.008418427, -0....\n",
       "4       [[-0.008891041, 0.010265926, 0.0072207046, -0....\n",
       "                              ...                        \n",
       "4995    [[-0.0007908391, 0.0063931425, 0.010211208, 0....\n",
       "4996    [[-0.013162706, 0.02190433, 0.007337241, -0.00...\n",
       "4997    [[-0.007895629853010178, 0.01972855255007744, ...\n",
       "4998    [[-0.0068212547339499, 0.01469762809574604, -4...\n",
       "4999    [[-0.0007908391, 0.0063931425, 0.010211208, 0....\n",
       "Name: arabic_embeddings, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['arabic_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b82e4a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[-0.005334263, 0.012371778, -0.0060608396, -0...\n",
       "1       [[0.005007278174161911, 0.00018593885761220008...\n",
       "2       [[0.0078026424, 0.00641275, -0.011493867, 0.00...\n",
       "3       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "4       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "                              ...                        \n",
       "4995    [[-0.009947495, 0.008167277, 0.0039029298, 0.0...\n",
       "4996    [[-0.009947495, 0.008167277, 0.0039029298, 0.0...\n",
       "4997    [[-0.009947494603693485, 0.008167277090251446,...\n",
       "4998    [[-0.009947495, 0.008167277, 0.0039029298, 0.0...\n",
       "4999    [[-0.009947495, 0.008167277, 0.0039029298, 0.0...\n",
       "Name: english_embeddings, Length: 5000, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['english_embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef0f1f",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba832f",
   "metadata": {},
   "source": [
    "### LSTM (seq2seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a5fc3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8baa5bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">693</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">178,101</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m693\u001b[0m)   │    \u001b[38;5;34m178,101\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">909,237</span> (3.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m909,237\u001b[0m (3.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">909,237</span> (3.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m909,237\u001b[0m (3.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the maximum sequence length for both input and output sequences\n",
    "max_encoder_seq_length = max(len(seq) for seq in dataset['arabic_preprocessed'])\n",
    "max_decoder_seq_length = max(len(seq) for seq in dataset['english_preprocessed'])\n",
    "\n",
    "# Define the input sequence\n",
    "encoder_inputs = Input(shape=(max_encoder_seq_length, arabic_model.vector_size))\n",
    "\n",
    "# Define the LSTM encoder\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the input sequence for the decoder\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length, english_model.vector_size))\n",
    "\n",
    "# Define the LSTM decoder\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Define the Dense layer for output\n",
    "decoder_dense = Dense(len(english_model.wv.key_to_index), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Print the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a317d44",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e766fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Function to pad or truncate embedding sequences to a fixed length\n",
    "def pad_embedding_sequence(embeddings, max_len, vector_size):\n",
    "    padded = np.zeros((max_len, vector_size))\n",
    "    for i in range(min(max_len, len(embeddings))):\n",
    "        padded[i] = embeddings[i]\n",
    "    return padded\n",
    "\n",
    "# Prepare encoder input data (Arabic embeddings)\n",
    "encoder_input_data = np.array([\n",
    "    pad_embedding_sequence(seq, max_encoder_seq_length, arabic_model.vector_size)\n",
    "    for seq in dataset['arabic_embeddings']\n",
    "])\n",
    "\n",
    "# Prepare decoder input data (English embeddings)\n",
    "decoder_input_data = np.array([\n",
    "    pad_embedding_sequence(seq, max_decoder_seq_length, english_model.vector_size)\n",
    "    for seq in dataset['english_embeddings']\n",
    "])\n",
    "\n",
    "# Tokenize English preprocessed text\n",
    "tokenizer_eng = Tokenizer()\n",
    "tokenizer_eng.fit_on_texts(dataset['english_preprocessed'])\n",
    "\n",
    "# Convert English sentences to integer sequences\n",
    "decoder_input_sequences = tokenizer_eng.texts_to_sequences(dataset['english_preprocessed'])\n",
    "\n",
    "# Prepare decoder target sequences using teacher forcing (shifted by one)\n",
    "decoder_target_sequences = [seq[1:] + [0] for seq in decoder_input_sequences]\n",
    "\n",
    "# Get vocabulary size for decoder output layer\n",
    "eng_vocab_size = len(tokenizer_eng.word_index) + 1\n",
    "\n",
    "# Pad decoder input and target sequences to uniform length\n",
    "decoder_input_sequences = pad_sequences(decoder_input_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "decoder_target_sequences = pad_sequences(decoder_target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "# One-hot encode decoder target data\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(decoder_target_sequences), max_decoder_seq_length, eng_vocab_size),\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "for i, seq in enumerate(decoder_target_sequences):\n",
    "    for t, word_id in enumerate(seq):\n",
    "        if word_id > 0:\n",
    "            decoder_target_data[i, t, word_id] = 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845809b",
   "metadata": {},
   "source": [
    "#  Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82797f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_11      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)      │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ lstm_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ lstm_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2789</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">716,773</span> │ lstm_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_10      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_11      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m100\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_10[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)      │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m365,568\u001b[0m │ input_layer_11[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ lstm_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ lstm_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m2789\u001b[0m)  │    \u001b[38;5;34m716,773\u001b[0m │ lstm_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,447,909</span> (5.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,447,909\u001b[0m (5.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,447,909</span> (5.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,447,909\u001b[0m (5.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 216ms/step - loss: 0.3533 - val_loss: 0.3460\n",
      "Epoch 2/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 204ms/step - loss: 0.3409 - val_loss: 0.3439\n",
      "Epoch 3/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 205ms/step - loss: 0.3436 - val_loss: 0.3432\n",
      "Epoch 4/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 205ms/step - loss: 0.3370 - val_loss: 0.3435\n",
      "Epoch 5/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 203ms/step - loss: 0.3408 - val_loss: 0.3430\n",
      "Epoch 6/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 207ms/step - loss: 0.3431 - val_loss: 0.3435\n",
      "Epoch 7/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 207ms/step - loss: 0.3393 - val_loss: 0.3436\n",
      "Epoch 8/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 203ms/step - loss: 0.3549 - val_loss: 0.3432\n",
      "Epoch 9/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 205ms/step - loss: 0.3440 - val_loss: 0.3436\n",
      "Epoch 10/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 207ms/step - loss: 0.3366 - val_loss: 0.3435\n",
      "Epoch 11/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 206ms/step - loss: 0.3338 - val_loss: 0.3432\n",
      "Epoch 12/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 208ms/step - loss: 0.3419 - val_loss: 0.3438\n",
      "Epoch 13/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 206ms/step - loss: 0.3363 - val_loss: 0.3432\n",
      "Epoch 14/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 207ms/step - loss: 0.3499 - val_loss: 0.3432\n",
      "Epoch 15/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 209ms/step - loss: 0.3400 - val_loss: 0.3436\n",
      "Epoch 16/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 208ms/step - loss: 0.3401 - val_loss: 0.3439\n",
      "Epoch 17/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 210ms/step - loss: 0.3495 - val_loss: 0.3434\n",
      "Epoch 18/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 213ms/step - loss: 0.3589 - val_loss: 0.3435\n",
      "Epoch 19/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 210ms/step - loss: 0.3461 - val_loss: 0.3435\n",
      "Epoch 20/20\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 208ms/step - loss: 0.3445 - val_loss: 0.3436\n",
      " Training complete. Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "#  Model Training (Seq2Seq with LSTM)\n",
    "# -----------------------------\n",
    "\n",
    "# Import necessary modules from Keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Define encoder input layer (Arabic input embeddings)\n",
    "encoder_inputs = Input(shape=(max_encoder_seq_length, arabic_model.vector_size))\n",
    "\n",
    "# Encoder LSTM: returns hidden and cell states\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Store the encoder states to initialize the decoder\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define decoder input layer (English input embeddings)\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length, english_model.vector_size))\n",
    "\n",
    "# Decoder LSTM: takes encoder states as initial state\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Dense layer to predict the next word from decoder outputs\n",
    "decoder_dense = Dense(eng_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the full Seq2Seq model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model using categorical crossentropy loss\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# Display the model architecture\n",
    "print(model.summary())\n",
    "\n",
    "# -----------------------------\n",
    "#  Train the model\n",
    "# -----------------------------\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "#  Save the trained model and tokenizer\n",
    "# -----------------------------\n",
    "model.save(\"arabic_to_english_translation_model.keras\")\n",
    "\n",
    "import pickle\n",
    "with open(\"english_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer_eng, f)\n",
    "\n",
    "print(\" Training complete. Model and tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e285b",
   "metadata": {},
   "source": [
    "# Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ba1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "#  Model Evaluation Section\n",
    "# -----------------------------\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Create reverse tokenizer to map indices back to words\n",
    "reverse_tokenizer_eng = {v: k for k, v in tokenizer_eng.word_index.items()}\n",
    "smoothie = SmoothingFunction().method1\n",
    "\n",
    "# Function to decode a sequence using the trained encoder and decoder\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1, english_model.vector_size))\n",
    "    decoded_sentence = []\n",
    "\n",
    "    for _ in range(max_decoder_seq_length):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_tokenizer_eng.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_word == '' or sampled_word == '<end>':\n",
    "            break\n",
    "\n",
    "        decoded_sentence.append(sampled_word)\n",
    "\n",
    "        # Update the target sequence with the new word embedding\n",
    "        if sampled_word in english_model.wv:\n",
    "            target_seq[0, 0, :] = english_model.wv[sampled_word]\n",
    "        else:\n",
    "            target_seq[0, 0, :] = np.zeros((english_model.vector_size,))\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "# Function to compute BLEU score for evaluation\n",
    "def evaluate_bleu_score(dataset, sample_size=100):\n",
    "    bleu_scores = []\n",
    "\n",
    "    for i in tqdm(range(sample_size)):\n",
    "        input_embedding = encoder_input_data[i:i+1]\n",
    "        predicted = decode_sequence(input_embedding)\n",
    "        reference = [dataset['english_preprocessed'][i]]\n",
    "        candidate = predicted.split()\n",
    "\n",
    "        score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "        # Print every 10 samples for visual reference\n",
    "        if i % 10 == 0:\n",
    "            print(f\"\\nArabic Input    : {' '.join(dataset['arabic_preprocessed'][i])}\")\n",
    "            print(f\"Reference Output: {' '.join(reference[0])}\")\n",
    "            print(f\"Model Prediction: {' '.join(candidate)}\")\n",
    "            print(f\"BLEU Score      : {score:.4f}\")\n",
    "\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "    print(f\"\\n Average BLEU Score on {sample_size} samples: {avg_bleu:.4f}\")\n",
    "    return avg_bleu\n",
    "\n",
    "# Function to compute classification metrics: accuracy, precision, recall, F1-score\n",
    "def classification_metrics(dataset, sample_size=100):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for i in tqdm(range(sample_size)):\n",
    "        input_embedding = encoder_input_data[i:i+1]\n",
    "        predicted = decode_sequence(input_embedding).split()\n",
    "        reference = dataset['english_preprocessed'][i]\n",
    "\n",
    "        # Pad sequences to equal length\n",
    "        max_len = max(len(predicted), len(reference))\n",
    "        pred_padded = predicted + [''] * (max_len - len(predicted))\n",
    "        ref_padded = reference + [''] * (max_len - len(reference))\n",
    "\n",
    "        for ref_word, pred_word in zip(ref_padded, pred_padded):\n",
    "            y_true.append(ref_word)\n",
    "            y_pred.append(pred_word)\n",
    "\n",
    "    # Convert words to numerical labels\n",
    "    all_words = list(set(y_true + y_pred))\n",
    "    word2idx = {word: i for i, word in enumerate(all_words)}\n",
    "    y_true_ids = [word2idx[word] for word in y_true]\n",
    "    y_pred_ids = [word2idx[word] for word in y_pred]\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true_ids, y_pred_ids)\n",
    "    precision = precision_score(y_true_ids, y_pred_ids, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_true_ids, y_pred_ids, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_true_ids, y_pred_ids, average='macro', zero_division=0)\n",
    "\n",
    "    print(\"\\n Classification Metrics:\")\n",
    "    print(f\" Accuracy : {accuracy:.4f}\")\n",
    "    print(f\" Precision: {precision:.4f}\")\n",
    "    print(f\" Recall   : {recall:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# --------- Run Evaluation ---------\n",
    "print(\" Evaluating model...\\n\")\n",
    "bleu = evaluate_bleu_score(dataset, sample_size=100)\n",
    "accuracy, precision, recall, f1 = classification_metrics(dataset, sample_size=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
